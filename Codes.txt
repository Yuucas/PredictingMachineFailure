--- LIBRARIES ---
import os
import numpy as np
import scipy
import pandas as pd
from functools import reduce
from dotenv import load_dotenv

# Preprocessing
from feature_engine.selection import RecursiveFeatureElimination

# Training
import torch
from pytorch_lightning.callbacks import EarlyStopping

from darts import TimeSeries
from darts.dataprocessing.transformers import Scaler
from darts.metrics import smape, mae, mse
from darts.models import NHiTSModel
from darts.utils.likelihood_models import QuantileRegression

# Data processing
from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder

# Visualization
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

# Model evaluation
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay

--- Load Dataset ---
load_dotenv() # Load .env variables

DATASET = os.getenv("DATASET")

# load dataset
dataset = pd.read_csv(DATASET)
print(dataset.head(5))

# Description of dataset
dataset.describe()

# the data types
dataset.info()

# Check if there are null or missing values
dataset.isna().sum()

# Check Type counts
dataset['Type'].value_counts()

dataset = dataset.drop(["UDI", "Product ID", "Failure Type"], axis=1)

--- FEATURE CREATION ---
# Temperature difference between process and air
dataset['temperature_difference'] = dataset['Process temperature [K]'] - dataset['Air temperature [K]']
# Mechanical power using torque and rotational speed
dataset['Mechanical Power [W]'] = np.round((dataset['Torque [Nm]']*dataset['Rotational speed [rpm]']* 2 * np.pi) / 60, 4)

# Statistical Description
dataset.describe().T

--- ENCODING COLUMNS ---
# Label encoding categorical variables (column- Type)
dataset['Type'] = LabelEncoder().fit_transform(dataset['Type'])

--- FEATURE ELIMINATION ---
# initialize linear regresion estimator
rfr_model = RandomForestRegressor()

# initialize feature selector
# neg_mean_squared_error
rfe = RecursiveFeatureElimination(estimator=rfr_model, scoring="r2", cv=3)

# Prepare dataset
X = dataset.drop(["Target"], axis=1)
y = dataset["Target"]

x_transformed = rfe.fit_transform(X, y)

--- DATASET PREPARATION FOR TRAINING ---
# Define which columns are your main features and which are covariates
main_feature_cols = ['Torque [Nm]', 'Tool wear [min]', 'temperature_difference', 'Mechanical Power [W]']
covariate_cols = ['Type']

# Convert DataFrame into Darts TimeSeries format
main_ts = TimeSeries.from_dataframe(x_transformed, value_cols=main_feature_cols)
covariates_ts = TimeSeries.from_dataframe(x_transformed, value_cols=covariate_cols)

# The 'Target' column (0 or 1) is what we want to predict.
target_ts = TimeSeries.from_series(y) # y is dataset["Target"]

print("--- Original TimeSeries (first 5 entries) ---")
print("Main TS:")
print(main_ts.to_dataframe().head())
print("\nCovariates TS:")
print(covariates_ts.to_dataframe().head())

# Split dataset
split_fraction = 0.8

train_main_ts, test_main_ts = main_ts.split_after(split_fraction)
train_target_ts, test_target_ts = target_ts.split_after(split_fraction)

if covariates_ts:
    train_covariates_ts, test_covariates_ts = covariates_ts.split_after(split_fraction)
else:
    train_covariates_ts, test_covariates_ts = None, None

print(f"\n--- Data Splitting ---")
print(f"Training main series length: {len(train_main_ts)}")
print(f"Test main series length: {len(test_main_ts)}")
print(f"Training covariates series length: {len(train_covariates_ts)}")
print(f"Test covariates series length: {len(test_covariates_ts)}")

# Scale the data
scaler = Scaler(StandardScaler())

# Fit the scaler only on the training part
scaler.fit(train_main_ts)

# Transform both the training and test data
train_main_ts_scaled = scaler.transform(train_main_ts)
test_main_ts_scaled = scaler.transform(test_main_ts)

# Prepare past_covariates for Training
if train_covariates_ts:
    train_past_covariates = train_main_ts_scaled.stack(train_covariates_ts)
else: # If no covariates
    train_past_covariates = train_main_ts_scaled

# Prepare full_past_covariates for Prediction
full_main_ts_scaled = scaler.transform(main_ts)

# Stack with the entire original covariates_ts (if it exists).
if covariates_ts:
    full_past_covariates = full_main_ts_scaled.stack(covariates_ts)
else:
    full_past_covariates = full_main_ts_scaled

print("--- Data Preparation for N-HiTS Complete ---")
print(f"Training Target TS length: {len(train_target_ts)}")
print(f"Training Past Covariates components: {train_past_covariates.n_components}, length: {len(train_past_covariates)}")
if full_past_covariates:
    print(f"Full Past Covariates components: {full_past_covariates.n_components}, length: {len(full_past_covariates)}")

--- TRAINING ---

# Train N-HiTS model
input_chunk_length = 100  # Number of past time steps the model sees. Adjust based on data.
output_chunk_length = 10 # Predicting 1 step ahead (failure at the next step).
n_epochs = 200           # Number of training epochs. Increase for real datasets.
num_stacks = 3
num_layers = 2
layer_widths = 128
num_blocks= 1
dropout = 0.1
lr = 1e-4
activation = "ReLU"
optimizer_class = torch.optim.Adam

# reproducibility
torch.manual_seed(42)

# Ensure training target series is long enough
if len(train_target_ts) < input_chunk_length + output_chunk_length:
    new_input_chunk_length = max(1, len(train_target_ts) - output_chunk_length -1)
    print(f"Warning: Training target series is very short ({len(train_target_ts)}).")
    print(f"Reducing input_chunk_length from {input_chunk_length} to {new_input_chunk_length}.")
    input_chunk_length = new_input_chunk_length
    if input_chunk_length <= 0 : # Changed from == 0 to <=0 for robustness
        raise ValueError(f"Training target series (length {len(train_target_ts)}) is too short for "
                         f"input_chunk_length {input_chunk_length} and output_chunk_length {output_chunk_length}. "
                         "Need more data or smaller chunk lengths.")

# Model parameter setup
early_stopper = EarlyStopping(monitor="train_loss", min_delta=0.001, patience=15, verbose=True, mode="min")
callbacks = [early_stopper]

pl_trainer_kwargs = {
    "accelerator": "cuda",
    "callbacks": callbacks,
    "enable_progress_bar": True
}


# learning rate scheduler
lr_scheduler_cls = torch.optim.lr_scheduler.ExponentialLR
lr_scheduler_kwargs = {"gamma": 0.999}

# build the TCN model
model = NHiTSModel(
    input_chunk_length=input_chunk_length,
    output_chunk_length=output_chunk_length,
    batch_size=32,
    num_stacks = num_stacks,
    num_blocks = num_blocks,
    num_layers = num_layers,
    layer_widths = layer_widths,
    activation = activation,
    n_epochs=n_epochs,
    nr_epochs_val_period=1,
    dropout=dropout,
    optimizer_kwargs={"lr": lr},
    likelihood=QuantileRegression(),
    pl_trainer_kwargs=pl_trainer_kwargs,
    model_name="NHits",
    force_reset=True,
    save_checkpoints=True,
)

model.fit(series=train_target_ts, past_covariates=train_past_covariates)

model = NHiTSModel.load_from_checkpoint("NHits", best=True)
print("\n--- N-HiTS Model Training Complete ---")

--- PREDICTION ---

# --- Making Predictions on the Test Set ---
if len(test_target_ts) > 0:
    print("\n--- Making Predictions on Test Set ---")
    try:
        # `series` = historical target data (train_target_ts) to condition the forecast start.
        # `past_covariates` = all available past covariates (full_past_covariates).
        # Darts will select the appropriate slice of `full_past_covariates`.
        predictions_nhits = model.predict(
            n=len(test_target_ts),
            series=train_target_ts,
            past_covariates=full_past_covariates
        )

        print("\nRaw Continuous Predictions (first 5 values):")
        print(predictions_nhits.to_dataframe().head(10))

        # --- Evaluation (for binary target) ---
        actual_values = test_target_ts.values(copy=True).flatten()
        predicted_continuous_values = predictions_nhits.values(copy=True).flatten()

        threshold = 0.5 # Common threshold for binary classification
        predicted_labels = (predicted_continuous_values >= threshold).astype(int)

        # Ensure lengths match for comparison (in case of any off-by-one issues due to prediction mechanics)
        min_len = min(len(actual_values), len(predicted_labels))
        actual_labels_eval = actual_values[:min_len]
        predicted_labels_eval = predicted_labels[:min_len]

        if min_len > 0:
            print(f"\n--- Evaluation (Threshold = {threshold}) ---")
            comparison_df = pd.DataFrame({
                'Actual': actual_labels_eval,
                'Predicted_Label': predicted_labels_eval,
                'Predicted_Continuous': predicted_continuous_values[:min_len]
            })
            print(comparison_df.head(10))

            
            print("\nClassification Report:")
            print(classification_report(actual_labels_eval, predicted_labels_eval, zero_division=0))
            print(f"Accuracy: {accuracy_score(actual_labels_eval, predicted_labels_eval):.4f}")
            try:
                roc_auc = roc_auc_score(actual_labels_eval, predicted_continuous_values[:min_len])
                print(f"ROC AUC Score: {roc_auc:.4f}")
            except ValueError as e:
                print(f"Could not calculate ROC AUC Score: {e}") #

            print("\nConfusion Matrix:")
            # Calculate the confusion matrix
            cm = confusion_matrix(actual_labels_eval, predicted_labels_eval)
            print("\nConfusion Matrix (raw array):")
            print(cm)

            # --- Plotting the Confusion Matrix as a Heatmap ---
            # Method 1: Using scikit-learn's ConfusionMatrixDisplay (Recommended)
            display_labels = [0, 1] # Assuming your labels are 0 (No Failure) and 1 (Failure)
            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)

            fig, ax = plt.subplots(figsize=(6, 5)) # Adjust figure size as needed
            disp.plot(ax=ax, cmap=plt.cm.Blues, values_format='d') # 'd' for integer format
            ax.set_title('N-HiTS Model - Confusion Matrix')
            # disp.ax_.set_title('N-HiTS Model - Confusion Matrix') # Alternative way to set title if needed
            plt.show()


             # --- Plotting Actual vs. Predicted Labels ---
            # Define the desired x-axis range for display
            x_min_display = 8000
            x_max_display = 8050 # Show up to index 8050

            plt.figure(figsize=(15, 7)) # Adjusted figure size for better zoomed view

            # Get the time index from the test_target_ts for the x-axis
            # This index corresponds to the original DataFrame's index for the test portion
            time_index_full_test = test_target_ts.time_index # This is a pandas Index

            # Ensure actual_labels_eval and predicted_labels_eval are numpy arrays for boolean indexing
            actual_labels_np = np.array(actual_labels_eval)
            predicted_labels_np = np.array(predicted_labels_eval)
            time_index_np = time_index_full_test[:min_len].to_numpy() # Convert relevant part of index to numpy

            # Create a mask for the desired display range
            # This assumes time_index_np contains the global indices you want to filter by
            display_mask = (time_index_np >= x_min_display) & (time_index_np <= x_max_display)

            # Filter the data using the mask
            time_index_display = time_index_np[display_mask]
            actual_labels_display = actual_labels_np[display_mask]
            predicted_labels_display = predicted_labels_np[display_mask]

            if len(time_index_display) > 0: # Check if there's any data in the selected range
                plt.plot(time_index_display, actual_labels_display, label='Actual Failures', marker='o', linestyle='-', color='blue', alpha=0.7, markersize=8)
                plt.plot(time_index_display, predicted_labels_display, label='Predicted Failures', marker='x', linestyle='--', color='red', alpha=0.7, markersize=8)

                plt.title(f'Actual vs. Predicted Machine Failures (Test Set: Index {x_min_display}-{x_max_display})')
                plt.xlabel('Time Step / Index')
                plt.ylabel('Failure (1) / No Failure (0)')
                plt.yticks([0, 1])
                plt.xticks(np.arange(min(time_index_display), max(time_index_display)+1, step=max(1, (max(time_index_display)-min(time_index_display))//10))) # Adjust x-ticks for better readability in zoom
                plt.xlim(x_min_display -1, x_max_display +1) # Set x-axis limits to focus on the range, with a little padding
                plt.legend()
                plt.grid(True, which='both', linestyle='--', linewidth=0.5)
                plt.tight_layout()
                plt.show()
            else:
                print(f"No data points found in the specified display range ({x_min_display}-{x_max_display}) within the evaluated test set.")

        else:
            print("Not enough data in the test set for evaluation after prediction, or lengths mismatch.")

    except Exception as e:
        print(f"Error during prediction or evaluation: {e}")
        import traceback
        traceback.print_exc()
else:
    print("\nTest set (test_target_ts) is empty. Skipping prediction and evaluation.")